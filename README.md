# BioRAG ‚Äî Hybrid RAG sobre Europe PMC (bioRxiv)

Este proyecto implementa un RAG h√≠brido (l√©xico + sem√°ntico) para:
1) buscar documentos en PMC,
2) rankear candidatos combinando BM25 + embeddings (FAISS),
3) extraer snippets cortos como evidencia,
4) generar una respuesta en ingl√©s con citas [n] usando un modelo local v√≠a Ollama.

---

## Pipeline (end-to-end)

### A. Entrada del usuario
- El usuario escribe una pregunta (en ingl√©s) en la UI (Gradio).

### B. Traducci√≥n/normalizaci√≥n de la consulta (para Europe PMC)
- Se convierte a una query en ingl√©s estilo Lucene (AND/OR/NOT, frases entre comillas, etc.).
- Esto se hace con un modelo local v√≠a Ollama y un prompt few-shot.

**Objetivo:** producir una query robusta para el buscador de Europe PMC.

### C. B√∫squeda en Europe PMC
- Se consulta el endpoint de PMC con la query generada.
- Se descargan metadatos (t√≠tulo, abstract, a√±o, DOI, links).

**Salida:** lista de documentos normalizados.

### D. Index local (BM25 + embeddings) + persistencia
Se construye un √≠ndice local para esos documentos:

1) **BM25 (similitud l√©xica)**
- Se tokenizan los textos (title + abstract) y se busca similitud por BM25.

2) **Embeddings + FAISS (similitud contextual)**
- Se embebe cada documento con SentenceTransformers.
- Se guardan/reusan embeddings en `./biorag_store/`:
  - `embeddings.npy` (vectores)
  - `meta.parquet` (metadatos + hash por texto normalizado)
- Con esos embeddings se construye un √≠ndice FAISS para b√∫squeda por similitud coseno.

**Objetivo:** no recalcular embeddings si el mismo texto ya exist√≠a (dedupe por hash).

### E. Ranking h√≠brido (score final)
Para rankear:
- Score l√©xico: BM25 sobre palabras clave extra√≠das desde la query Lucene.
- Score contextual: FAISS sobre embedding de la pregunta original.
- Score final: combinaci√≥n lineal
  - `alpha * score_vector + (1 - alpha) * score_bm25`

### F. Extracci√≥n de evidencia (snippets)
- Para los top docs, se arma un snippet por documento (m√°x. N snippets).
- Se cortan oraciones del t√≠tulo+abstract y se priorizan oraciones con patrones deseables (p-values, etc.).
- Cada snippet queda corto (ej. <=200 chars) para entrar f√°cil en el contexto del generador.

### G. Generaci√≥n (answer + referencias) con citas [n]
- Se construye un prompt con:
  - lista de snippets como contexto, cada l√≠nea termina con su cita [n]
  - instrucci√≥n: responder en ingl√©s y **solo** con evidencia del contexto, citando cada afirmaci√≥n con [n]
- Se llama a Ollama con un modelo local (por defecto: `gemma3:1b`).
- Se devuelve:
  - `answer` (con citas inline [n])
  - `refs` (lista formateada de referencias [n] con t√≠tulo, fuente, a√±o, link/DOI)

---

## Estructura del c√≥digo

- `orquestador.py`:
  - Une todo el pipeline y expone una UI en Gradio (entrada: ‚ÄúConsulta‚Äù, salida: query final, respuesta con citas, referencias y top recuperados).
- `rag_index.py`:
  - Traducci√≥n de consulta a Lucene, b√∫squeda en PMC, normalizaci√≥n de resultados.
  - `Index_coincidencias`: BM25 + embeddings + FAISS + score h√≠brido.
- `vector_store.py`:
  - Persistencia de embeddings y metadatos en `./biorag_store/` (reuso + dedupe).
- `retriever.py`:
  - Split simple en oraciones y selecci√≥n de snippets representativos.
- `generador.py`:
  - Arma el prompt con citas y llama a Ollama de forma robusta (generate/chat).
  - Devuelve respuesta + bloque de referencias.

---

## C√≥mo correr
1) Instalar dependencias:
   - `pip install -r requirements.txt`
2) Tener Ollama corriendo y con modelos descargados:
   - Query: `codellama:7b-instruct`
   - Answer: `gemma3:1b`
3) Ejecutar:
   - `python orquestador.py`
4) Entrar a la interfaz local en:
üëâ http://127.0.0.1:7862


Notas:
- Par√°metros √∫tiles: `page_size`, `alpha`, `top_k`, `max_snippets`.
- Persistencia: se crea `./biorag_store/` autom√°ticamente.

## Arquitectura del RAG

![Arquitectura del RAG](img/rag_pipeline.png)

